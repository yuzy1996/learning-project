# 高性能并行 

## 1.多线程

对应技术：threading（多线程），利用CPU和IO可以同时执行的原理，避免CPU干巴巴等待IO完成；asyncio（异步IO），在单线程利用CPU和IO同时执行的原理，实现函数异步执行。

## 2.多cpu

对应技术：multiprocessing（多进程），利用多核心CPU功能，真正执行并行任务。

### 3.多机器

对应技术: hadoop/hive/spark

## 4.向量化

向量化，比方说在矩阵相乘的运算中，普通的做法是一个循环分别取一行和一列里面的一个数，做乘加运算。如果先把行和列的数集中放在一个比较宽的寄存器里面，比如512位的寄存器可以放16个32位的单精度浮点数，一个乘加指令就能完成对这16个数的计算，从而得到16倍的速度提高。



## 优势

如果不利用多核来写多线程的程序，以及线程中没有利用向量化指令来做运算，例如实测使用英特尔至强CPU的服务器，2007年和2014年相比，跑单线程并且没有使用向量化指令的程序，性能几乎没有增长。而经过多线程并行化和向量化调优之后，性能就会有102倍的提高了。

从发展趋势来看，高性能运算的机器越来越普及，越来越碎片化。最早从巨型机，分布式处理机、向量机，到最后走向集群，而现在出现GPGPU之后，单台服务器，甚至是一台笔记本的性能就可以媲美几年前看起来很强大的高性能运算中心。高性能并行计算不再远在天边，而在你的指尖。第二个变化是异构架构的普及，协处理器，GPGPU，FPGA等高性能处理部件飞入寻常百姓家。在这样两个大趋势下，高性能和并行计算编程、调优不再是一个很遥远的事情，而是一个必须每天要面对的事情。





## 并行计算编程常用的有两个技术

### OpenMP技术

针对单台服务器，准确地说是共享内存系统，充分利用多核、多线程的并行处理能力，通常使用OpenMP技术。对于大量的数据做类似的处理的应用，通常在编程中使用计算密集循环来完成数据处理。这个循环一般就可以通过OpenMP 技术，添加编译器指导指令使其自动变成一个多线程程序，每个线程处理其中一部分数据，在执行完以后自动把结果收拢起来，得到最终结果，这样就能充分利用多核的处理性能了。

### MPI技术

对于问题规模超过单节点处理能力的应用，可以使用MPI技术。利用很多台机器同时运算，比如天河二号上面有的应用需要使用上百万个核做处理，显然不可能有一台机器可以拥有100万个核，那么当使用这么多台机器一起处理数据的时候，一个重要的问题就是要通过网络互联来交换数据。如果自己从底层开始编写网络交换代码是不现实的，你会发现大部分时间都在调试各种网络问题，没有时间写算法了。而有一个通用的标准API把它封装起来，做这些底层工作，这个API就叫做MPI，用于发送信息和接收信息，实现数据交换。

异构程序的开发也比几年前好了很多。英特尔的KNL的开发就跟普通的CPU编程没有区别，针对GPGPU编程也可以使用CUDA或者OpenCL，这些都大幅度降低了开发的难度。

## 高性能和并行计算优化基础

### 阿曼达定律

阿曼达定律说的是，如果一个程序包括并行和串行，随着机器数量增加，并行执行时间会越来越短，最后趋向于0，串行的时间没有变，这就是加速比，如果串行部分占到了整个执行时间的50%，意味着加到1024台机器也只能加速一倍，非常浪费，哪怕并行的部分占到90%，其实也就是加速了9倍。这个讲得是当工作量是固定的时候，可以并行处理的部分所占比例越高越好，描述的是程序的强可扩展性特性。

### Gustafson定律

Gustafson定律说得是，在不断增加处理工作量的情形下，增加系统的规模是有用的。比如天气预报计算，如果增加了1万倍的处理量，又必须在规定时间之内算完，怎么办。很多情况下，工作规模增加的部分都是可以并行处理的部分，因此增加系统规模自然会缩短处理时间。这是一个非常乐观的定律，也就是说汇聚了很多机器一定能干更大的事。

阿曼达定律描述了随着增加更多的处理器，串行部分处理没变，只是并行部分的执行时间会不断减少，但总的加速比是受串行部分所占比例限制的。而Gustafson定律则描述了随着工作量的增加，加上更多的处理器单元，可以缩短并行处理的时间，从而在规定时间内，处理的工作量增加了。

## 调优的方法论：

调优是一个看起来很庞杂、很细致的工作。需要一个方法论做指导，才会事半功倍。调优过程是这样一个循环，首先要找到一个典型的、可重复的测试用例，然后使用这个测试用例得到一个基准,记录这个基准。在基准的测试过程中，需要收集大量的性能数据，这些性能数据将会指出问题所在。

系统的性能遵循木桶原理，也就是整体性能是由系统中最短的那块板决定的，在性能数据收集的过程中可以通过性能指标发现哪个地方有问题，根据具体问题就可以找到相应的解决方案来解决。使用解决方案解决这个问题之后，再重新测试收集，找到下一块短板。以此循环往复，直至性能达到期望或者无法继续增进为止。

整个优化应该采用自上而下的方法，顺序一定不能乱。首先通过标准性能基准测试程序确保系统的工作状态正常， 比如使用SpecInt， SpecFP，Linpack等得到处理器的性能，对比设计性能指标，可以得知CPU是否工作正常，BIOS或者操作系统中的相关设置是否正确。使用Stream测试程序测试内存系统的性能，Netperf测试网络性能，Fio或者iozone等测试文件系统性能是否正常。在所有调优开始之前，一定要先把基础做好，一定要了解你的系统性能极限。

再来是应用调优，可以调节运行环境，或者有代码的可以调整代码，最后才会到处理器级别的调优，这里是榨干最后一滴性能的地方。

比如说我发现内存访问是一个问题，第一个步骤是检查硬件设置，服务器的内存插法是有黑魔法的，怎么插，插在不同位置上，性能是不一样的，BIOS内存访问方式设置也会有比较大的影响，可以翻看相关服务器的产品手册。

另外内存控制器是有不同通道的，每个通道速度不一样，而且一般来讲当内存所有通道都被插满内存条的时候，内存会被自动降频，因此每CPU使用单条大内存的性能会超过同样内存总容量下，所有内存插槽都插满了的小内存的速度。当服务器有多个处理器存在的时候，通常是一个非对称统一内存地址访问的架构（NUMA架构），也就是说每个CPU的内存控制器都有可能挂着自己的内存，它离得最近，别的CPU就远，因此内存是有距离的。BIOS中设置成对称多处理（SMP），内存编址将会混合远近不同的物理内存地址，达到均匀访问的目的，但是会损失性能。而现在几乎所有的主流操作系统都是NUMA 感知的，能充分利用NUMA架构，尽可能使用近距离内存以提高性能。

硬件调整做完之后还有软件调整，比如操作系统里调整内存页的大小，缺省的4K一个的小页在海量内存容量的情况下，意味着页表条目非常巨大。会增加DTLB Miss的机率，导致系统忙于装载切换DTLB和内存页，从而损失性能。

在应用代码中最主要相关的是数据结构和算法。比如说，某一点的坐标float X，Y，Z，有大量的点需要处理，而有时候会用不同的方式分别处理X，Y，Z，假设定义一个结构，包含X，Y，Z三个值，然后再定义一个结构的数组，每个元素是一个结构，如果这么处理的话，处理X值的任务是怎么访问内存的呢？隔一段距离跳着访问，内存访问的效率是很低的，尤其是高速缓存的利用率，还会产生所谓假共享（false sharing）的性能问题。

这里有一个改进办法，就是定义一个结构，包含三个数组，所有X放在一个数组，所有Y放在一个数组，所有Z放在一个数组。它有几个好处，首先，这样所有的X值就连续放在一起了，尽管在程序中访问内存的时候是可以按字节存取的，但是CPU在实际执行的时候是以高速缓存线（cache line）最小单位存取的，缓存线一般是64个字节大小，第一次读取X的第一个数的时候，实际上一次就取64个字节上去，第二个数也被读取了，因此访问第二个数的时候，在高速缓存线中，可以大幅度节省时间。还有因为特别有规律，一个一个连取，CPU内部会自动预取数据，它会在使用之前就把数据准备好，这个数据结构的更改会大幅度提高性能。

同样因为按高速缓存线为单位存取的原因，假如处理过程中都需要修改X，Y，Z的值，在原来基于结构的数组的情况下，处理X数据的任务，修改了X值，会导致同一个点的Y值在高速缓存中也被标记为修改了，因为它们在同一个高速缓存线中，会导致处理Y的任务，在读取Y值的时候，被迫刷新高速缓存线，从内存中重新读取数据，这就是所谓的假共享问题，会导致性能急剧下降。

## 高性能调优的方法分类：硬件级、运行级、编译器级和代码级

![](C:\Users\Administrator\Pictures\Camera Roll\性能优化.png)

### （一）硬件级

性能优化的方法有很多种，第一个叫硬件级调优，就是简单粗暴直接换掉性能低的硬件，比如网卡从千兆换到万兆的，硬盘从机械的换成SSD等等。很多时候这也不失为一个好办法。

### （二）运行级

另外一个所谓运行级调优，是从运行环境上调整，通过监控整个系统的性能及各项指标看问题所在，然后看能不能通过一些运行参数的调整，比如说内存的使用率非常高，可以试试在操作系统中调整内存页的大小。如果是网络带宽压力特别大，可以试试将网络包的处理程序绑定在某一个核上面。对于网络小包特别多的情况，有一些网卡带包聚合功能，等很多小包会聚合到一定的程度，再统一处理，大量减少中断数量，降低系统消耗。这些调整的成本很低，难点在于对技术人员要求很高，需要对整个系统非常熟。

### （三）编译器级

还有编译器级调优，需要有代码，但是不修改代码，使用编译器的优化选项，有的时候也能够获得巨大的性能提高。比如引入自动向量化，深度优化，性能剖析指导的优化（PGO）等等。需要技术人员熟悉编译的使用，以及对优化过程的理解。

### （四）代码级

还有代码级调优，也就是直接改动实现代码，改代码收益或许非常大，比如换个算法，因为有的算法就是比别的算法快很多倍，甚至快几个数量级，但是修改的难度极其大，成本极高，代码改了以后正确性要重新验证，所有测试的步骤都要做。

在原有串行单线程程序中，如果有比较明显的计算密集型循环，可以引入OpenMP进行并行化，结合编译器的自动向量化编译选项，可以只改极小一部分代码，获得比较大的性能收益。

黄新平先生紧接着介绍了在性能调优中常用的一些关键指标：系统性能指标包括CPU利用率、内存使用率，swap分区、带宽使用率；处理器性能指标包括CPI、浮点运算的峰值、向量化比例、cache miss、DTLB miss等。

第一个，CPU利用率，CPU利用率其实内部包括了几个方面，光看利用率数值是不够的，比如CPU利用率100%，但是system time占了30%，这样其实性能并不好，user time是CPU跑用户程序的时间，越高越好，而system time 是消耗在内核里面的时间，通常是由网络、磁盘的一些中断导致的，更常见的是锁。还有一个是io wait，当大量磁盘读取的时候就会有比较明显的占比。

还有一个是swap的使用， 当使用到swap的时候会感觉很慢，几乎没有响应了，这是为什么？是因为硬盘的访问及其缓慢。 大家通常对特别小的数据没有太多概念。一般来讲，CPU取第一级Cache典型的时间是十来个时钟周期，如果按2G赫兹算的话是几个纳秒，而硬盘访问的延迟大概是几十个毫秒。有一个很形象的比喻，如果从一级缓存取数需要1秒的时间的话，那么从硬盘中读取数据就需要116天。

另外一个是网络，网络分为千兆网、万兆网、infiniband网络，这个千兆网就是1000比特每秒，万兆就是10000比特每秒。infiniband是一个非常快速的网络，它能达到40G到100G。

到了CPU里面，有一个重要的指标叫CPI（Cycles Pre Instruction），就是每条指令需要多少个时钟周期完成，现在主流处理器的理论值是0.25，因为它有4个算术逻辑运算单元。实际上测下来，比如Linpack能够达到0.33。为什么CPI会升高，因为指令有内存访问的时候它不得不等待，虽然乱序执行能够掩盖一部分延迟，但是不可能完全填补运算速度与内存访问直接的差距，这个指标值就上来了。

另外一个指标是浮点运算的峰值，如果大家关心过GPU的一般都会知道这个数，每秒钟执行多少浮点型运算。实际运行的浮点型运算的性能值和理论峰值之间的比值就是浮点运算单元的使用率，很不幸的是并行科技运维过很多计算中心，发现绝大多数应用的这个使用率小于10%， 也就是说买了一个160公里时速的车永远以10多公里的时速开， 问题基本都在软件层面上。有时候优化过后，就可以达到60%到70%多，一下子性能提升了六七倍。

向量化比例是刚才浮点型运算效率偏低的原因，因为没用向量化，还在用单个标量运算，肯定性能比较烂，利用向量化之后，一条指令执行16-32条数据就处理完了，而标量计算才处理一个，很多时候就是改一改编译器的选项就完成了。

Cache miss比例是指数据访问的时候没有在高速缓存里找到的，需要到内存中去取，肯定会带来较大的性能损失，这个miss比率对性能的影响比较大, 性能好的时候是很低的数据，一般要小于10%。

还有一个是DTLB miss。进程访问的地址空间是逻辑地址空间，比如64位的程序，地址范围从0到2的64次方减一，然而这么大的地址空间，实际上不可能配置这么多物理内存的，需要建立一个从逻辑内存到物理内存的映射表，这个映射表的管理是按照页为最小单位的，缺省的页大小是4K。但是页大小可调，越大的页，页表条目越少，页大小为4M的跟4k的页表条目就差了1024倍。这些页表不可能完全放在CPU里面，CPU只能放在一个称为DTLB的缓冲区中，更多的部分放在内存里面，当映射相应条目的时候，如果在缓冲区找不到就会产生DTLB miss的异常，这个异常处理会从内存中找到相关条目，然后放入DTLB缓冲区。这个操作极其耗时，经常发生时对性能影响极大，通常可以通过调整页大小，或者在程序中使用内存池等内存管理技术减少耗时。

还有像内存带宽，内存带宽体现了内存访问的忙闲程度，这个值高到一定程度，会导致内存延迟迅速增加，有一些工具比如并行的Paramon和Intel的VTune可以帮助测量这个值。

## 并行应用调优工具

工欲善其事必先利其器，工具按照刚才调优的级别会有不同，一个是系统级的分析，包括CPU、GPU、网络、内存、swap等。一是应用级分析，包括代码中各函数的耗时、MPI计算与通信占比等。

并行科技的Paramon应用性能收集监控工具，会实时收集大量性能指标，实时显示并分析常见性能问题，并将这些数据记录下来，供Paratune应用性能分析工具进行详细分析，Paratune可以实时回放集群各服务器性能指标，并且将这一段的运行特征分析出来，以四个象限展示，是CPU密集型、内存访问密集型、磁盘密集型还是网络密集型一目了然。这是并行最早的产品，现在这一块作为HPC云计算服务的一个基础的工具存在，我们给客户服务的时候全是拿这些东西做分析。建立了一个大量的运行特征库，可以直接指导应用的调优方向及匹配的硬件服务器的选型与采购。

英特尔的工具VTune，这是一个调优神器，实际用起来操作很简单，难在它给出了一大堆报告和数据之后怎么样解读它，怎么样利用它。它可以迅速定位应用中最耗时的部分以及发现并行程序中不正确的同步导致的性能损失。直接对应到代码帮助调优者迅速找到原因。

### 代码级性能优化即代码现代化的方法

代码级的性能优化，有一个非常好的，就是称为代码现代化的一系列方法。 最主要的有并行化，就是要多线程化，充分利用多核资源；另外一个是向量化，充分利用处理器向量位宽，实现单指令多数据的处理；还有是内存访问优化，在KNL或者GPU这样的有高速高带宽内存的时候，需要充分利用这些资源，将最常访问的数据与代码放入高带宽内存区域，缩小处理器处理速度与内存访问速度之间的差距。

## 并行应用调优实战

### 矩阵相乘程序阐述编译参数的调整进行性能调优。

不改变代码，仅利用编译器参数进行调优。为我们演示了2048乘以2048的矩阵相乘的算法。仅仅通过编译器编译选项的调整就可以大幅度提高运行效率。执行时间从25秒缩短到0.14秒，性能提高178倍，却不需要改一行代码。

### 通过Black Sholes算法程序的调优过程演示了少量改变部分代码的方式进行性能调优。

Black Sholes算法分了两个部分，第一个部分用随机数产生器产生大量随机数，模拟买卖单。 第二个部分利用模拟数据计算当前的投资价值。首先用VTune寻找问题， 先找热点，发现指数运算函数、对数运算函数加上一个随机数产生器函数被大量调用。而且它是一个单线程的程序，所以第一件事就是在模拟计算部分的计算密集的for循环处加了OpenMP编译指令，同时使用编译器的自动向量化编译选项，获得了4倍的性能提升。继续考察程序，发现初始化部分的随机数产生器， 在英特尔的MKL库里有一个非常好的实现，因此可以直接换上这个实现，最终总体程序获得了22.8倍的性能提升。

演示中跑50万个模拟，三次迭代。基准性能数据是平均1472个时钟周期做一个模拟，初始化用了1073个时钟周期，模拟计算花了399个时钟周期。使用OpenMP和向量化指令优化后， 总时间变成了364个时钟周期，初始化用了338个，计算用了26个。换成MKL库的随机数生成函数后，总时间变成了64， 初始化用了35，计算用了29。在一个双核的笔记本上，计算时间从1472变成64，22.8倍的提升，实际代码量改动非常小。如果在更多核的服务器上将会有更大的性能提升。
